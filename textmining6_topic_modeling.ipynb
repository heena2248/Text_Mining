{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQVIobuCijjv4YWyQohLIV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heena2248/Text_Mining/blob/main/textmining6_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic Modeling is used to discover hidden semantic patterns in a text corpus and thus identify topics in the corpus. The techniques demonstrated are:\n",
        " - LSA\n",
        " - LDA\n",
        " - NMF\n",
        " - PCA\n",
        " - ICA\n"
      ],
      "metadata": {
        "id": "w5GelnmAAdWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSA and LDA"
      ],
      "metadata": {
        "id": "oLgv3o9SVJym"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtdXMWqGUuGO"
      },
      "outputs": [],
      "source": [
        "doc_1 = \"A whopping 96.5 percent of water on Earth is in our oceans, covering 71 percent of the surface of our planet. And at any given time, about 0.001 percent is floating above us in the atmosphere. If all of that water fell as rain at once, the whole planet would get about 1 inch of rain.\"\n",
        "\n",
        "doc_2 = \"One-third of your life is spent sleeping. Sleeping 7-9 hours each night should help your body heal itself, activate the immune system, and give your heart a break. Beyond that--sleep experts are still trying to learn more about what happens once we fall asleep.\"\n",
        "\n",
        "doc_3 = \"A newborn baby is 78 percent water. Adults are 55-60 percent water. Water is involved in just about everything our body does.\"\n",
        "\n",
        "doc_4 = \"While still in high school, a student went 264.4 hours without sleep, for which he won first place in the 10th Annual Great San Diego Science Fair in 1964.\"\n",
        "\n",
        "doc_5 = \"We experience water in all three states: solid ice, liquid water, and gas water vapor.\"\n",
        "\n",
        "# Create corpus\n",
        "corpus = [doc_1, doc_2, doc_3, doc_4, doc_5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "# remove stopwords, punctuation, and normalize the corpus\n",
        "stop = set(stopwords.words('english'))\n",
        "exclude = set(string.punctuation)\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "def clean(doc):\n",
        "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
        "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
        "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
        "    return normalized\n",
        "\n",
        "clean_corpus = [clean(doc).split() for doc in corpus]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K74SPSPUU8Df",
        "outputId": "a17b30b5-b223-4b53-8477-5adb13568b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "\n",
        "# Creating document-term matrix\n",
        "dictionary = corpora.Dictionary(clean_corpus)\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_corpus]"
      ],
      "metadata": {
        "id": "trfQWT0YU_7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LsiModel\n",
        "\n",
        "# LSA model\n",
        "lsa = LsiModel(doc_term_matrix, num_topics=3, id2word = dictionary)\n",
        "\n",
        "# LSA model\n",
        "print(lsa.print_topics(num_topics=3, num_words=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An9ci4swVGI9",
        "outputId": "c952b7fd-a11a-4775-dee1-2fe4497c0856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '0.555*\"water\" + 0.489*\"percent\" + 0.239*\"rain\"'), (1, '-0.361*\"sleeping\" + -0.215*\"still\" + -0.215*\"hour\"'), (2, '-0.562*\"water\" + 0.231*\"rain\" + 0.231*\"planet\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get topic distribution for each document\n",
        "# This gives the distribution of each document across topics\n",
        "doc_topic_distributions = [lsa[doc] for doc in doc_term_matrix]\n",
        "\n",
        "# Display the topic distribution for each document\n",
        "for i, topic_dist in enumerate(doc_topic_distributions):\n",
        "    print(f\"\\nDocument {i}:\")\n",
        "    for topic, contribution in topic_dist:\n",
        "        print(f\"  Topic {topic}: {contribution:.4f}\")\n",
        "\n",
        "# Find the dominant topic for each document\n",
        "for i, topic_dist in enumerate(doc_topic_distributions):\n",
        "    dominant_topic = max(topic_dist, key=lambda x: x[1])  # max by contribution\n",
        "    print(f\"Document {i} is most related to Topic {dominant_topic[0]} with a contribution of {dominant_topic[1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjyrHjkZ-P5B",
        "outputId": "1081548b-f31f-4ec0-a3bb-c17c85662358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 0:\n",
            "  Topic 0: 5.9267\n",
            "  Topic 1: 0.3096\n",
            "  Topic 2: 2.3749\n",
            "\n",
            "Document 1:\n",
            "  Topic 0: 0.1583\n",
            "  Topic 1: -5.3127\n",
            "  Topic 2: 0.2414\n",
            "\n",
            "Document 2:\n",
            "  Topic 0: 3.2349\n",
            "  Topic 1: -0.2740\n",
            "  Topic 2: -2.6690\n",
            "\n",
            "Document 3:\n",
            "  Topic 0: 0.0104\n",
            "  Topic 1: -1.0183\n",
            "  Topic 2: 0.3135\n",
            "\n",
            "Document 4:\n",
            "  Topic 0: 1.9863\n",
            "  Topic 1: -0.0489\n",
            "  Topic 2: -2.7604\n",
            "Document 0 is most related to Topic 0 with a contribution of 5.9267\n",
            "Document 1 is most related to Topic 2 with a contribution of 0.2414\n",
            "Document 2 is most related to Topic 0 with a contribution of 3.2349\n",
            "Document 3 is most related to Topic 2 with a contribution of 0.3135\n",
            "Document 4 is most related to Topic 0 with a contribution of 1.9863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LdaModel\n",
        "\n",
        "# LDA model\n",
        "lda = LdaModel(doc_term_matrix, num_topics=3, id2word = dictionary)\n",
        "\n",
        "# Results\n",
        "print(lda.print_topics(num_topics=3, num_words=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAt1ATkEVRA_",
        "outputId": "54ac19dd-8719-4bf8-b007-f922fdba32ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '0.030*\"sleeping\" + 0.025*\"hour\" + 0.022*\"still\"'), (1, '0.038*\"percent\" + 0.032*\"rain\" + 0.030*\"planet\"'), (2, '0.087*\"water\" + 0.046*\"percent\" + 0.019*\"body\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get topic distribution for each document\n",
        "# This gives the distribution of each document across topics\n",
        "doc_topic_distributions = [lda.get_document_topics(doc) for doc in doc_term_matrix]\n",
        "\n",
        "# Display the topic distribution for each document\n",
        "for i, topic_dist in enumerate(doc_topic_distributions):\n",
        "    print(f\"\\nDocument {i}:\")\n",
        "    for topic, contribution in topic_dist:\n",
        "        print(f\"  Topic {topic}: {contribution:.4f}\")\n",
        "\n",
        "# Find the dominant topic for each document\n",
        "for i, topic_dist in enumerate(doc_topic_distributions):\n",
        "    dominant_topic = max(topic_dist, key=lambda x: x[1])  # max by contribution\n",
        "    print(f\"Document {i} is most related to Topic {dominant_topic[0]} with a contribution of {dominant_topic[1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "telljDyZVsst",
        "outputId": "5fad5f43-5a55-43ce-9658-1a3b841bb480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 0:\n",
            "  Topic 0: 0.0112\n",
            "  Topic 1: 0.9756\n",
            "  Topic 2: 0.0132\n",
            "\n",
            "Document 1:\n",
            "  Topic 0: 0.9750\n",
            "  Topic 1: 0.0125\n",
            "  Topic 2: 0.0125\n",
            "\n",
            "Document 2:\n",
            "  Topic 0: 0.0225\n",
            "  Topic 1: 0.0230\n",
            "  Topic 2: 0.9545\n",
            "\n",
            "Document 3:\n",
            "  Topic 0: 0.9629\n",
            "  Topic 1: 0.0170\n",
            "  Topic 2: 0.0201\n",
            "\n",
            "Document 4:\n",
            "  Topic 0: 0.0280\n",
            "  Topic 1: 0.0283\n",
            "  Topic 2: 0.9436\n",
            "Document 0 is most related to Topic 1 with a contribution of 0.9756\n",
            "Document 1 is most related to Topic 0 with a contribution of 0.9750\n",
            "Document 2 is most related to Topic 2 with a contribution of 0.9545\n",
            "Document 3 is most related to Topic 0 with a contribution of 0.9629\n",
            "Document 4 is most related to Topic 2 with a contribution of 0.9436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NMF"
      ],
      "metadata": {
        "id": "b9eSbIkUpgdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF"
      ],
      "metadata": {
        "id": "mwRl_MPUpm6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Preprocess and vectorize the text\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "tEgvjy6z-rv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Apply NMF\n",
        "num_topics = 2  # Specify the number of topics you want to extract\n",
        "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
        "nmf_model.fit(tfidf)\n",
        "\n",
        "# Step 3: Display the topics\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(f\"Topic {topic_idx + 1}:\")\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "no_top_words = 5  # Number of top words to display\n",
        "display_topics(nmf_model, tfidf_vectorizer.get_feature_names_out(), no_top_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3WNbDP2pjQx",
        "outputId": "d158868c-1288-40ee-c1e3-bae36a124635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1:\n",
            "water percent rain planet experience\n",
            "Topic 2:\n",
            "sleeping sleep hours 10th went\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA and ICA"
      ],
      "metadata": {
        "id": "GQ9Y_QL7p95y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA, FastICA"
      ],
      "metadata": {
        "id": "k6CsL5NxpsbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(tfidf.toarray())\n",
        "\n",
        "# Display PCA components\n",
        "print(\"PCA Components:\")\n",
        "for i, component in enumerate(pca.components_):\n",
        "    print(f\"Component {i + 1}:\")\n",
        "    top_indices = component.argsort()[-5:][::-1]  # Top 5 words\n",
        "    top_words = [tfidf_vectorizer.get_feature_names_out()[index] for index in top_indices]\n",
        "    print(\"Top words:\", top_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZZQBWwUqALj",
        "outputId": "2de43b6f-d3ed-4bc9-f6e5-7d2c41fcb1aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA Components:\n",
            "Component 1:\n",
            "Top words: ['water', 'percent', 'planet', 'rain', 'ice']\n",
            "Component 2:\n",
            "Top words: ['sleeping', 'help', 'spent', 'night', 'immune']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Apply ICA\n",
        "ica = FastICA(n_components=2, random_state=42)\n",
        "ica_result = ica.fit_transform(tfidf.toarray())\n",
        "\n",
        "# Display ICA components\n",
        "print(\"\\nICA Components:\")\n",
        "for i, component in enumerate(ica.components_):\n",
        "    print(f\"Component {i + 1}:\")\n",
        "    top_indices = component.argsort()[-5:][::-1]  # Top 5 words\n",
        "    top_words = [tfidf_vectorizer.get_feature_names_out()[index] for index in top_indices]\n",
        "    print(\"Top words:\", top_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-vcfVIiqDpw",
        "outputId": "7f1f40fa-4986-4a99-e7fe-e86f263f8c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ICA Components:\n",
            "Component 1:\n",
            "Top words: ['water', 'percent', 'planet', 'rain', 'ice']\n",
            "Component 2:\n",
            "Top words: ['sleeping', 'help', 'learn', 'life', 'immune']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS Tagging"
      ],
      "metadata": {
        "id": "gE1cD8H1YFO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "# Download the necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"The quick lazy fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "# Display the POS tags\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "id": "NAK0Xd45qGnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fc5b79-2cd2-488b-fd10-a5d71f707505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('lazy', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AYj1JKyBYRjm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}